# 1.기울기 소실과 폭주(Gradient Vanishing & Exploding)

# 기울기 소실 완화하는 가장 간단한 방법
# 은닉층의 활성화 함수로 시그모이드, 탄젠트가 아닌 ReLU, Leaky ReLU 사용

# -------------------------------------------------------------------

# 2.가중치 초기화

# 세이비어 초기화
# 정규분포, 균등분포로 나뉨
# 여러 층의 기울기 분산 사이에 균형을 맞춰 특정 층이 너무 주목받거나 다른 층 뒤쳐지는 것을 막음
# 시그모이드, 탄젠트 같은 S자 형태의 활성화 함수와는 좋은 성능
# ReLU와는 성능 좋지 않음

# He 초기화
# 정규분포, 균등분포로 나뉨
# 다음 층의 뉴런의 수를 반영하지 않음
# ReLU 계열과 효율적
# ReLU + He 초기화 방법이 보편적

# -------------------------------------------------------------------

# 3.배치 정규화

# 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동평균, 이동분산을 저장
# 테스트 시 구해놓았던 평균과 분산으로 정규화

# 가중치 초기화에 덜 민감해짐
# 학습률 증가 -> 학습 속도 개선
# 모델을 복잡하게 해 추가계산을 하는 것 -> 테스트데이터에 대한 예측 시 느려짐

# 배치 정규화의 한계
# 미니 배치 크기에 의존적
# RNN에 적용하기 어려움

# -------------------------------------------------------------------

# 4.층 정규화
#'https://wikidocs.net/images/page/61375/%EC%B8%B5%EC%A0%95%EA%B7%9C%ED%99%94.PNG'