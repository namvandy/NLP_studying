# 워드투벡터(Word2Vec)
# http://w.elnn.kr/search/ 한국어 단어에 대해 벡터 연산가능한 사이트

# 분산 표현
# 희소표현에서 단어간 유사성 표현 불가 -> 단어의 '의미'를 다차원 공간에 벡터화 하는 방법
# 분포 가설의 가정 하에 만들어짐. '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'
# Word2Vec으로 임베딩 된 벡터는 굳이 벡터의 차원이 단어 집합의 크기가 될 필요없음
# 사용자가 설정한 차원을 가지는 벡터가 되며 실수형의 값을 가짐

# CBOW(Continuous Bag of Words), Skip-Gram
# CBOW - 주변의 단어들을 가지고 중간의 단어를 예측
# SKip-Gram - 중간의 단어로 주변 단어를 예측
# 예문 : "The fat cat sat on the mat"
'''
# CBOW #
{"The", "fat", "cat", "on", "the", "mat"} 로부터 sat 예측
예측단어 sat = 중심 단어
예측에 사용하는 단어 = 주변 단어
중심 단어 예측 위해 앞,뒤 몇 개의 단어를 볼지를 결정한 범위 = 윈도우
윈도우크기를 결정 -> 윈도루를 계속 움직이며 주변단어와 중심단어선택을 변경하며 학습을 위한 데이터 셋 생성 => 슬라이싱 윈도우
입력층의 입력으로 윈도우 크기범위 안에 있는 주변단어들의 원-핫벡터가 들어감
출력층은 예측하고자 하는 중간 단어의 원-핫 벡터 필요
# Word2Vec은 은닉층이 개만 있는 DNN이 아닌 SNN(Shallow Neural Network)
# 은닉층은 활성화 함수 존재 x, 룩업 테이블이라는 연산을 담당하는 층으로 투사층(projection layer)이라고도 부름
#Input layer - Projection layer
cat(one-hot vector) - W_(V*M)
on(one-hot vector) - W_(V*M) 은 투사층의 크기 M 존재
M은 임베딩하고 난 벡터의 차원 - > M=5이면 CBOW 수행 후 각 단어의 임베딩 벡터의 차원은 5

#Projectiong layer - Output layer
입력,투사층 사이의 가중치 W는 V * M행렬
투사,출력층 사이의 가중치 W'는 M * V 행렬
이 두 행렬은 동일한 행렬을 transpose한 것이 아닌 서로 다른 행렬.
CBOW는 주변 단어로 중심단어를 맞추기위해 W와 W'를 학습해가는 구조

# 입력으로 들어오는 주변단어의 원-핫 벡터와 가중치 W행렬의 곱 이루어지는 과정
주변단어의 원-핫벡터: x  / 입력벡터: 원-핫벡터
i번쨰 인덱스에 1, 나머지 0값 가지는 입력벡터와 가중치 W행렬의 곱 = W행렬의 i번째 행을 그대로 읽어옴 => 룩업 테이블
lookup해온 W의 각 행벡터가 사실 Word2Vec을 수행 후 각 단어의 M차원의 크기를 가진 임베딩 벡터들임

주변단어의 원-핫 벡터 * 가중치 W = 결과벡터들 -> 투사층에서 만나 이 벡터들의 평균인 벡터를 구하게 됨.
-> 투사층에서 벡터의 평균을 구하는 건 Skip-Gram가 다른 부분

이렇게 구해진 평균벡터는 두번째 가충치 행렬 W'와 곱해짐 -> 차원이 V로 동일한 벡터

이 벡터에 CBOW는 softmax -> 0~1사이, 각 원소의 총 합은 1 -> 스코어벡터
스코어벡터의 j번쨰 인덱스가 가진 0~1사이의 값 -> j번째 단어가 중심 단어일 확률 -> 실제로 값을 아는 중심단어 원-핫벡터의 값에 가까워 져야함
스코어벡터=y_hat , 중심단어=y -> 두 벡터의 오차를 줄이기 위해 CBOW는 손실함수로 cross-entropy 사용
이제 역전파 수행-> W와 W'학습 -> M차원의 크기를 갖는 W의 행이나 W'의 열로부터  어떤 것을 임베딩 벡터로 사용할지를 결정
때로는 W와 W"의 평균치로 임베딩 벡터 선택
'''
'''
Skip-Gram
중심단어에서 주변단어를 예측
투사층에서 벡터의 평균 구하는 과정 X
여러 논문의 성능비교결과 -> Skip-Gram이 CBOW보다 성능 좋음
'''
'''
Negative Sampling
대체적으로 Word2Vec은 SGNS(Skip-Gram with Negative Sampling)을 사용

Word2Vec의 문제점: 속도
소프트맥스 -> 오차 구함 -> 모든 단어의 임베딩 조정 -> 단어가 중심단어, 주변단어와 관계없어도 수행 -> 굉장히 무거운 작업
전체 단어집합이 아닌 일부단어 집합에 대해서만 고려
주변단어집합과 주변단어가 아닌 상관없는 단어집합 생성 -> 훨씬 작은 단어 집합 -> 마지막 단계를 이진분류로 진행 -> 효율적가능 
'''